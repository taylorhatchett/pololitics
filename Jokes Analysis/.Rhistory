load("~/Downloads/pollstR-master/pollsterAPI.R")
install.packages("pollsteR")
install.packages("pollstR")
library("pollstR", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
dat <- pollstR(chart="2016-president-gop-primary",pages="all")
all_charts <- pollstr_charts()
pollstr_polls(topic='2016-president')
gov <- pollstr_charts(topic='2016-president')
install.packages("hitandrun")
install.packages("homeR")
install.packages("Lahman")
install.packages("Sabermetrics")
library("Sabermetrics", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
summary(tempChange5)
#for, all years
#for, all years
rm(list=ls())
rm(list=ls())
wpage = readLines('http://data.giss.nasa.gov/gist…/tabledata_v3/GLB.Ts+dSST.txt')
yearList = vector()
for(i in 1:length(wpage)){
if(grepl("([1-2][0-9][0-9][0-9])",substr(wpage[i],0,4),perl=T)){
yearList=c(yearList,wpage[i])}}
year = vector();jan = vector();feb = vector();mar = vector()
apr = vector();may = vector();jun = vector();jul = vector()
aug = vector();sep = vector();oct = vector();nov = vector()
dec = vector()
for(i in 1:length(yearList)){
year = c(year,as.numeric(substr(yearList[i],0,4)))
jan = c(jan,as.numeric(substr(yearList[i],8,10)))
feb = c(feb,as.numeric(substr(yearList[i],13,15)))
mar = c(mar,as.numeric(substr(yearList[i],18,20)))
apr = c(apr,as.numeric(substr(yearList[i],23,25)))
may = c(may,as.numeric(substr(yearList[i],28,30)))
jun = c(jun,as.numeric(substr(yearList[i],33,35)))
jul = c(jul,as.numeric(substr(yearList[i],38,40)))
aug = c(aug,as.numeric(substr(yearList[i],43,45)))
sep = c(sep,as.numeric(substr(yearList[i],48,50)))
oct = c(oct,as.numeric(substr(yearList[i],53,55)))
nov = c(nov,as.numeric(substr(yearList[i],58,60)))
dec = c(dec,as.numeric(substr(yearList[i],63,65)))}
tempData<-as.data.frame(cbind(year,jan,feb,mar,apr,may,jun,jul,aug,sep,oct,
nov,dec))
tempData <- reshape(tempData,varying=c("jan","feb","mar","apr","may",
"jun","jul","aug","sep","oct","nov","dec"),v.names = "temp",
timevar = "month", time = c("jan","feb","mar","apr","may",
"jun","jul","aug","sep","oct","nov","dec"), direction="long")
*#for, all years
tempChangeAll <- lm(temp~year + factor(month), data=tempData)
summary(tempChangeAll)
#for last 20 years
tempData20 <- subset(tempData,year>1995)
tempChange20 <- lm(temp~year + factor(month), data=tempData20)
summary(tempChange20)
#for last 10 years
tempData10 <- subset(tempData,year>2005)
tempChange10 <- lm(temp~year + factor(month), data=tempData10)
summary(tempChange10)
#for last 5 years
tempData5 <- subset(tempData,year>2010)
tempChange5 <- lm(temp~year + factor(month), data=tempData5)
summary(tempChange5)
summary(tempChange5)
summary(tempChange5)
summary(tempChange5)
source('~/Documents/Untitled.R')
rm(list=ls())
wpage = readLines('http://data.giss.nasa.gov/gist…/tabledata_v3/GLB.Ts+dSST.txt')
yearList = vector()
for(i in 1:length(wpage)){
if(grepl("([1-2][0-9][0-9][0-9])",substr(wpage[i],0,4),perl=T)){
yearList=c(yearList,wpage[i])}}
year = vector();jan = vector();feb = vector();mar = vector()
apr = vector();may = vector();jun = vector();jul = vector()
aug = vector();sep = vector();oct = vector();nov = vector()
dec = vector()
for(i in 1:length(yearList)){
year = c(year,as.numeric(substr(yearList[i],0,4)))
jan = c(jan,as.numeric(substr(yearList[i],8,10)))
feb = c(feb,as.numeric(substr(yearList[i],13,15)))
mar = c(mar,as.numeric(substr(yearList[i],18,20)))
apr = c(apr,as.numeric(substr(yearList[i],23,25)))
may = c(may,as.numeric(substr(yearList[i],28,30)))
jun = c(jun,as.numeric(substr(yearList[i],33,35)))
jul = c(jul,as.numeric(substr(yearList[i],38,40)))
aug = c(aug,as.numeric(substr(yearList[i],43,45)))
sep = c(sep,as.numeric(substr(yearList[i],48,50)))
oct = c(oct,as.numeric(substr(yearList[i],53,55)))
nov = c(nov,as.numeric(substr(yearList[i],58,60)))
dec = c(dec,as.numeric(substr(yearList[i],63,65)))}
tempData<-as.data.frame(cbind(year,jan,feb,mar,apr,may,jun,jul,aug,sep,oct,
nov,dec))
tempData <- reshape(tempData,varying=c("jan","feb","mar","apr","may",
"jun","jul","aug","sep","oct","nov","dec"),v.names = "temp",
timevar = "month", time = c("jan","feb","mar","apr","may",
"jun","jul","aug","sep","oct","nov","dec"), direction="long")
tempChangeAll <- lm(temp~year + factor(month), data=tempData)
summary(tempChangeAll)
tempData20 <- subset(tempData,year>1995)
tempChange20 <- lm(temp~year + factor(month), data=tempData20)
summary(tempChange20)
tempData10 <- subset(tempData,year>2005)
tempChange10 <- lm(temp~year + factor(month), data=tempData10)
summary(tempChange10)
tempData5 <- subset(tempData,year>2010)
tempChange5 <- lm(temp~year + factor(month), data=tempData5)
summary(tempChange5)
install.packages(gwascat)
install.packages("genoPlotR")
install.packages("rsnps")
datalist <- allphenotypes()
allgensnp(snp='rs7412')[1:3]
library("rsnps", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
allgensnp(snp='rs7412')[1:3]
allphenotypes(df=TRUE)[1:10,]
datalist <- allphenotypes()
names(datalist)[1:10]
datalist[["ADHD"]]
genome_Taylor_Hatchett_Full_20130330125044 <- read.delim("~/Dropbox/Meh/genome_Taylor_Hatchett_Full_20130330125044.txt", header=FALSE, comment.char="#")
View(genome_Taylor_Hatchett_Full_20130330125044)
install.packages(c("ade4", "BH", "boot", "class", "cluster", "curl", "DBI", "digest", "dplyr", "foreign", "ggplot2", "gtable", "hexbin", "homeR", "httr", "jsonlite", "Lahman", "lazyeval", "MASS", "Matrix", "mgcv", "mime", "munsell", "nlme", "nnet", "pitchRx", "plyr", "pollstR", "R6", "rcdd", "Rcpp", "RCurl", "Sabermetrics", "scales", "spatial", "stringi", "survival", "XML"))
install.packages("Sabermetrics")
library("Sabermetrics", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
library("pitchRx", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
library(dplyr)
db <- src_sqlite("pitchfx.sqlite3", create = T)
scrape(start = "2016-04-03", end = Sys.Date(), connect = db$con)
library(dplyr)
db <- src_sqlite("pitchfx.sqlite3", create = T)
files <- c("miniscoreboard.xml", "players.xml", "inning/inning_hit.xml")
scrape(start = "2016-04-03", end = Sys.Date(), suffix = files, connect = db$con)
library(pitchRx)
scrape(start = "2016-04-03", end = Sys.Date(), suffix = files, connect = db$con)
install.packages("tm")
download.file("http://cran.cnr.berkeley.edu/src/contrib/Archive/Rstem/Rstem_0.4-1.tar.gz", "Rstem_0.4-1.tar.gz")
install.packages("Rstem_0.4-1.tar.gz", repos=NULL, type="source")
download.file("http://www.omegahat.net/Rstem/Rstem_0.4-1.tar.gz", "Rstem_0.4-1.tar.gz")
install.packages("Rstem_0.4-1.tar.gz", repos=NULL, type="source")
install.packages("~/Documents/Rstem_0.4-1.tar.gz", repos = NULL, type = "source")
library("tm", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
install.packages("Rstem", repos = "http://www.omegahat.org/R")
install.packages("Rstem", repos = "http://www.omegahat.org/Rstem")
install.packages("~/Downloads/Rstem_0.4-1.tar.gz", repos = NULL, type = "source")
install.packages("sentimentr")
install.packages(“tm”)
install.packages("tm")
download.file("http://cran.r-project.org/src/contrib/Archive/sentiment/sentiment_0.2.tar.gz", "sentiment.tar.gz")
install.packages(“Rstem_0.4-1.tar.gz”, repos=NULL, type=”source”)
install.packages("Rstem_0.4-1.tar.gz", repos=NULL, type="source")
library("Rstem", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
download.file("http://cran.r-project.org/src/contrib/Archive/sentiment/sentiment_0.2.tar.gz", "sentiment.tar.gz")
install.packages("sentiment.tar.gz", repos=NULL, type="source")
library(twitteR)
library(ROAuth)
require(RCurl)
library(stringr)
library(tm)
library(ggmap)
library(dplyr)
library(plyr)
library(tm)
library(wordcloud)
install.packages("~/Downloads/slam_0.1-38.tar.gz", repos = NULL, type = "source")
install.packages("countrycode")
library("countrycode", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
Jokes...csv <- read.csv("~/Documents/pololitics/Jokes Analysis/Jokes - csv.csv")
View(Jokes...csv)
library("NLP", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
install.packages("rgeos")
install.packages("rgeoapi")
install.packages("rworldmap")
install.packages("qdapDictionaries")
Sys.setenv(SPACY_PYTHON="/usr/local/bin/python")
devtools::install_github("kbenoit/spacyr")
install.packages("devtools")
devtools::install_github("kbenoit/spacyr")
install.packages(c("assertthat", "boot", "curl", "DBI", "foreign", "jsonlite", "lattice", "MASS", "memoise", "openxlsx", "readxl", "rpart", "survival", "tibble"))
install.packages(c("assertthat", "boot", "curl", "DBI", "foreign",
library("spacyr", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
txt <- Jokes...csv
spacy_initialize()
spacy_initialize(lang = "en")
detach("package:spacyr", unload=TRUE)
library("spacyr", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
spacy_initialize(lang = "en")
spacy_parse(txt)
View(Jokes...csv)
View(txt)
spacy_parse(txt.JOKE)
txt.JOKE
txt[JOKE]
txt["JOKE"]
txt <- Jokes...csv["JOKE"]
spacy_parse(txt)
txt[1]
txt.row
for (i in txt) {}
for (i in txt) {spacy_parse(i)}
spacy_parse(i)
install.packages("cleanNLP")
library("cleanNLP", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
mydata <- read.table(Jokes...csv, colClasses = c("joke", "key", "date", "comedian"))
mydata <- read.table('~/Documents/pololitics/Jokes Analysis/Jokes - csv.csv', colClasses = c("joke", "key", "date", "comedian"))
setwd("~/Documents/pololitics/Jokes Analysis")
mydata <- read.table('Jokes - csv.csv', colClasses = c("joke", "key", "date", "comedian"))
read.table('Jokes - csv.csv')
mydata <- read.table('Jokes - csv.xlsx', colClasses = c("joke", "key", "date", "comedian"))
mydata <- read.table('Jokes - xlsx.xlsx', colClasses = c("joke", "key", "date", "comedian"))
mydata <- read.table('Jokes - xlsx.xlsx', header = TRUE, colClasses = c("joke", "key", "date", "comedian"))
mydata <- read_csv('Jokes - csv.csv')
View(mydata)
value <- mydata$JOKE
for (i in value) {spacy_parse(i)}
parse <- for (i in value) {spacy_parse(i)}
value[i]
value
annotate(value,output_dir='~/Desktop',as_strings=TRUE)
init_spaCy()
install.packages("reticulate")
init_spaCy()
annotate(value,output_dir='~/Desktop',as_strings=TRUE)
library("NLP", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
install.packages(c("openNLP", "openNLPdata"))
library("maps", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
library("plyr", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
data("world.cities")
cvalue <- gsub(value, "[[:punct:]]", "")
cvalue <- gsub(txt, "[[:punct:]]", "")
for (jokes in value) {gsub(jokes, "[[:punct:]]", "")}
cvalue <- gsub(value[1:53447], "[[:punct:]]", "")
testing <- as.vector(as.matrix(txt))
testing[1:3]
c(t(testing))
testing1 <- c(t(testing))
df <- data.frame(Reduce(txt))
df <- data.frame(Reduce(rbind, txt))
View(df)
convert_text_to_sentences <- function(text, lang = "en") {
# Function to compute sentence annotations using the Apache OpenNLP Maxent sentence detector employing the default model for language 'en'.
sentence_token_annotator <- Maxent_Sent_Token_Annotator(language = lang)
# Convert text to class String from package NLP
text <- as.String(text)
# Sentence boundaries in text
sentence.boundaries <- annotate(text, sentence_token_annotator)
# Extract sentences
sentences <- text[sentence.boundaries]
# return sentences
return(sentences)
}
reshape_corpus <- function(current.corpus, FUN, ...) {
# Extract the text from each document in the corpus and put into a list
text <- lapply(current.corpus, Content)
# Basically convert the text
docs <- lapply(text, FUN, ...)
docs <- as.vector(unlist(docs))
# Create a new corpus structure and return it
new.corpus <- Corpus(VectorSource(docs))
return(new.corpus)
}
current.corpus <- Corpus(VectorSource(txt))
library("openNLP", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
current.corpus <- Corpus(VectorSource(txt))
library("tm", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
current.corpus <- Corpus(VectorSource(txt))
reshape_corpus(current.corpus, convert_text_to_sentences)
df <- data.frame(txt)
View(df)
df <- data.frame(txt,stringsAsFactors = FALSE)
View(df)
library("data.table", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
